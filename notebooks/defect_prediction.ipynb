{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Defect Prediction using PyTorch\n",
    "\n",
    "This notebook implements multi-label defect prediction models using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, hamming_loss, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Since we don't have a real dataset for defect prediction, we'll generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define defect types\n",
    "defect_labels = ['Security', 'Performance', 'Maintainability', 'Reliability', 'Functional']\n",
    "num_classes = len(defect_labels)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 20\n",
    "\n",
    "# Feature names\n",
    "feature_names = [\n",
    "    \"LOC\", \"Cyclomatic Complexity\", \"Nesting Depth\", \"Comment Density\",\n",
    "    \"Code Churn\", \"Coupling\", \"Cohesion\", \"Unique Operands\",\n",
    "    \"Unique Operators\", \"Branch Count\", \"Loop Count\", \"Parameter Count\",\n",
    "    \"Fan-in\", \"Fan-out\", \"Halstead Difficulty\", \"Halstead Volume\",\n",
    "    \"Halstead Effort\", \"Dependency Count\", \"Age\", \"Dev Experience\"\n",
    "]\n",
    "\n",
    "# Generate features\n",
    "X = np.random.rand(num_samples, num_features) * 10  # Scale to make more realistic\n",
    "\n",
    "# Generate labels (multi-label)\n",
    "y = np.zeros((num_samples, num_classes))\n",
    "for i in range(num_samples):\n",
    "    # Each sample has a 30% chance of having each defect type\n",
    "    for j in range(num_classes):\n",
    "        y[i, j] = 1 if np.random.rand() < 0.3 else 0\n",
    "    \n",
    "    # Ensure at least one defect type is present in 60% of samples\n",
    "    if np.random.rand() < 0.6 and np.sum(y[i]) == 0:\n",
    "        y[i, np.random.randint(0, num_classes)] = 1\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_df = pd.DataFrame(y, columns=defect_labels)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Feature data sample:\")\n",
    "display(X_df.head())\n",
    "\n",
    "print(\"\\nLabel data sample:\")\n",
    "display(y_df.head())\n",
    "\n",
    "print(f\"\\nData shapes: X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "# Count label distribution\n",
    "label_counts = y_df.sum()\n",
    "print(\"\\nLabel distribution:\")\n",
    "display(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the DNN model for multi-label classification\n",
    "class MultiLabelDNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size=128, dropout_rate=0.3):\n",
    "        super(MultiLabelDNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Create a flexible architecture\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.layer1(x)))\n",
    "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to train a PyTorch model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to evaluate a PyTorch model for multi-label classification\n",
    "def evaluate_model(model, test_loader, defect_labels):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Convert outputs to predictions\n",
    "            probs = outputs.cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds)\n",
    "            y_prob.extend(probs)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    hl = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    results = {}\n",
    "    for i, label in enumerate(defect_labels):\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred[:, i], average='binary'\n",
    "        )\n",
    "        results[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    # Calculate micro and macro averages\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true.flatten(), y_pred.flatten(), average='binary'\n",
    "    )\n",
    "    \n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Hamming Loss: {hl:.4f}\")\n",
    "    print(f\"Micro-average Precision: {precision_micro:.4f}\")\n",
    "    print(f\"Micro-average Recall: {recall_micro:.4f}\")\n",
    "    print(f\"Micro-average F1: {f1_micro:.4f}\")\n",
    "    print(f\"Macro-average Precision: {precision_macro:.4f}\")\n",
    "    print(f\"Macro-average Recall: {recall_macro:.4f}\")\n",
    "    print(f\"Macro-average F1: {f1_macro:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for label, metrics in results.items():\n",
    "        print(f\"{label}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (label, cm) in enumerate(zip(defect_labels, conf_matrices)):\n",
    "        if i < len(axes):\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                        xticklabels=['No Defect', 'Defect'],\n",
    "                        yticklabels=['No Defect', 'Defect'])\n",
    "            axes[i].set_title(f'{label} Defects')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('True')\n",
    "    \n",
    "    # Remove the last subplot if not needed\n",
    "    if len(defect_labels) < len(axes):\n",
    "        fig.delaxes(axes[-1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'hamming_loss': hl,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'per_class': results,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train and evaluate the DNN model\n",
    "input_size = X_train_scaled.shape[1]\n",
    "dnn_model = MultiLabelDNN(input_size, num_classes).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(dnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training DNN model...\")\n",
    "dnn_model = train_model(dnn_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating DNN model...\")\n",
    "dnn_results = evaluate_model(dnn_model, test_loader, defect_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze feature importance using model weights\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    # Get the weights from the first layer\n",
    "    weights = model.layer1.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Calculate the absolute importance of each feature\n",
    "    importance = np.abs(weights).mean(axis=0)\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_df = analyze_feature_importance(dnn_model, feature_names)\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs('../models/defect', exist_ok=True)\n",
    "\n",
    "# Save the DNN model\n",
    "torch.save(dnn_model.state_dict(), '../models/defect/dnn_model.pth')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, '../models/defect/scaler.joblib')\n",
    "\n",
    "# Save the defect labels\n",
    "with open('../models/defect/defect_labels.txt', 'w') as f:\n",
    "    for label in defect_labels:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(\"Models, scaler, and labels saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
